{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import*\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created\n",
      "Spark Version: 4.0.2\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session local mode\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pyspark-coding\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Total records: 2946\n",
      "Total columns: 28\n"
     ]
    }
   ],
   "source": [
    "# Load NYC Jobs dataset\n",
    "file_path = r\"file:///C:/Users/ASUS Zenbook 14 OLED/Workspace/Code_base/dataset/nyc-jobs.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"Total records: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data:\n",
      "+------+--------------------+------------+--------------+--------------------+--------------------+-------------+-----+--------------------+-----------------------------+-----------------+---------------+----------------+--------------------+--------------------+--------------------+-------------------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------------+---------------------+--------------------+----------+--------------------+--------------------+--------------+-------------+---------------+-------------------+------------+\n",
      "|Job ID|              Agency|Posting Type|# Of Positions|      Business Title| Civil Service Title|Title Code No|Level|        Job Category|Full-Time/Part-Time indicator|Salary Range From|Salary Range To|Salary Frequency|       Work Location|  Division/Work Unit|     Job Description|Minimum Qual Requirements|    Preferred Skills|Additional Information|            To Apply|         Hours/Shift|     Work Location 1|Recruitment Contact|Residency Requirement|        Posting Date|Post Until|     Posting Updated|        Process Date|Average_Salary|Salary_Bucket|Employment_Type|        format_date|Posted_since|\n",
      "+------+--------------------+------------+--------------+--------------------+--------------------+-------------+-----+--------------------+-----------------------------+-----------------+---------------+----------------+--------------------+--------------------+--------------------+-------------------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------------+---------------------+--------------------+----------+--------------------+--------------------+--------------+-------------+---------------+-------------------+------------+\n",
      "|339370|ADMIN FOR CHILDRE...|    Internal|             1|Court Liaison Off...|CHILD PROTECTIVE ...|        52366|    2|       Legal Affairs|                            F|          51315.0|        54720.0|          Annual|150 William Stree...|Family Court Lega...|Family Court Lega...|     1. A baccalaureat...|Preferred candida...|  Section 424-A of ...|\"Click \"\"Apply No...|                NULL|                NULL|               NULL| New York City Res...|2018-03-27T00:00:...|      NULL|2019-04-03T00:00:...|2019-12-17T00:00:...|       53017.5|    Mid Level|      Full-Time|2018-03-27 00:00:00|        2875|\n",
      "|340435|DEPT OF HEALTH/ME...|    Internal|             1|Supervising Healt...| PUBLIC HEALTH NURSE|        51011|    2|              Health|                            F|          73576.0|        73576.0|          Annual|90-27 Sutphin Blv...|SH Nursing Servic...|**OPEN TO PERMANE...|     1. A Bachelorâ€™s...|Expertise in Plan...|  **Must follow all...|Apply online with...|                NULL|                NULL|               NULL| New York City Res...|2018-04-09T00:00:...|      NULL|2018-04-09T00:00:...|2019-12-17T00:00:...|       73576.0|    Mid Level|      Full-Time|2018-04-09 00:00:00|        2862|\n",
      "|235309|DEPT OF HEALTH/ME...|    External|             1|Pest Control Aide...|   PEST CONTROL AIDE|        90500|    0|Public Safety, In...|                            F|          26457.0|        32665.0|          Annual|158 E 115Th St., ...|Pest Control Serv...|The Bureauof Vete...|     There are no form...|Knowledge of pest...|                  NULL|Apply online with...|                NULL|                NULL|               NULL| New York City res...|2016-03-14T00:00:...|      NULL|2016-06-08T00:00:...|2019-12-17T00:00:...|       29561.0|  Entry Level|      Full-Time|2016-03-14 00:00:00|        3618|\n",
      "|274845|DEPT OF ENVIRONME...|    Internal|             1|Administrative En...|ADM ENGINEER (NON...|        1001A|    0|Engineering, Arch...|                            F|          90000.0|       105000.0|          Annual|16 Little Hollow ...|Rondout Neversink...|The New York City...|     A valid New York ...|                NULL|  DEP is an equal o...|Click the 'Apply ...|35 Hours per week...|16 Little Hollow ...|               NULL| New York City Res...|2016-12-19T00:00:...|      NULL|2016-12-19T00:00:...|2019-12-17T00:00:...|       97500.0| Senior Level|      Full-Time|2016-12-19 00:00:00|        3338|\n",
      "|302670|DEPT OF ENVIRONME...|    External|             1|Assistant Civil E...|ASSISTANT CIVIL E...|        20210|    0|Engineering, Arch...|                            F|          53134.0|        79726.0|          Annual|Boro Hall Richmon...|Permit Control Se...|The New York City...|     1. A baccalaureat...|Excellent communi...|  Appointments are ...|\"Click the \"\"Appl...|                NULL|Boro Hall Richmon...|               NULL| New York City Res...|2017-08-30T00:00:...|      NULL|2017-09-18T00:00:...|2019-12-17T00:00:...|       66430.0|    Mid Level|      Full-Time|2017-08-30 00:00:00|        3084|\n",
      "|343034|DEPT OF ENVIRONME...|    External|             1| Landscape Architect| LANDSCAPE ARCHITECT|        21315|    1|Engineering, Arch...|                            F|          63074.0|        91347.0|          Annual|329 Greenpoint Av...|Dept of Environme...|The New York City...|     1.  A valid New Y...|                NULL|  â€œNOTE: This pos...|\"Click the \"\"appl...|                NULL|                NULL|               NULL| New York City Res...|2018-05-02T00:00:...|      NULL|2018-05-02T00:00:...|2019-12-17T00:00:...|       77210.5|    Mid Level|      Full-Time|2018-05-02 00:00:00|        2839|\n",
      "|349213|DEPT OF ENVIRONME...|    External|             3|WATERSHED MAINTAINER|WATERSHED MAINTAINER|        91011|    0|Building Operatio...|                            F|          38197.0|        55870.0|          Annual|182 Joline Ave, S...|      Blue Belt Unit|The New York City...|     A four-year high ...|                NULL|  Appointments are ...|\"Click the \"\"Appl...|                NULL|                NULL|               NULL| New York City res...|2018-06-25T00:00:...|      NULL|2018-07-16T00:00:...|2019-12-17T00:00:...|       47033.5|  Entry Level|      Full-Time|2018-06-25 00:00:00|        2785|\n",
      "|199768|DEPT OF INFO TECH...|    Internal|             1|  Payroll Supervisor| COMMUNITY ASSOCIATE|        56057|    0|Clerical & Admini...|                         NULL|          33799.0|        56249.0|          Annual|75 Park Place New...|     Human Resources|DoITT provides fo...|     \"Qualification Re...|The successful ca...|                  NULL|For DoITT Employe...|                NULL|              NY, NY|               NULL| New York City res...|2015-07-14T00:00:...|      NULL|2015-07-14T00:00:...|2019-12-17T00:00:...|       45024.0|  Entry Level|  Not Specified|2015-07-14 00:00:00|        3862|\n",
      "|299768|DEPT OF ENVIRONME...|    Internal|             1|Deputy Chief, Per...|ADM ENGINEER (NON...|        1001A|    0|Engineering, Arch...|                            F|          49990.0|       120000.0|          Annual|96-05 Horace Hard...|W   S/Connections...|The New York City...|     A valid New York ...|                NULL|  ALL CANDIDATES MU...|\"Click the \"\"Appl...|                NULL|                NULL|               NULL| New York City Res...|2017-08-15T00:00:...|      NULL|2017-08-15T00:00:...|2019-12-17T00:00:...|       84995.0| Senior Level|      Full-Time|2017-08-15 00:00:00|        3099|\n",
      "|311799|DEPARTMENT OF TRA...|    Internal|             1|         Electrician|         ELECTRICIAN|        91717|    0|Building Operatio...|                            F|           373.03|         373.03|           Daily| 88-26 Pitkin Avenue|Facilities Manage...|*** IN ORDER TO B...|     (1) Five years of...|                NULL|  *** IN ORDER TO B...|All resumes are t...|                NULL|   55 Water St Ny Ny|               NULL| New York City res...|2017-11-01T00:00:...|      NULL|2017-11-02T00:00:...|2019-12-17T00:00:...|        373.03|  Entry Level|      Full-Time|2017-11-01 00:00:00|        3021|\n",
      "+------+--------------------+------------+--------------+--------------------+--------------------+-------------+-----+--------------------+-----------------------------+-----------------+---------------+----------------+--------------------+--------------------+--------------------+-------------------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------------+---------------------+--------------------+----------+--------------------+--------------------+--------------+-------------+---------------+-------------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Show sample data on a dataframe\n",
    "print(\"\\nSample Data:\")\n",
    "\n",
    "df.show(10, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical columns:\n",
      "  - Job ID\n",
      "  - # Of Positions\n",
      "  - Salary Range From\n",
      "  - Salary Range To\n",
      "\n",
      "String columns:\n",
      "  - Agency\n",
      "  - Posting Type\n",
      "  - Business Title\n",
      "  - Civil Service Title\n",
      "  - Title Code No\n",
      "  - Level\n",
      "  - Job Category\n",
      "  - Full-Time/Part-Time indicator\n",
      "  - Salary Frequency\n",
      "  - Work Location\n",
      "  - Division/Work Unit\n",
      "  - Job Description\n",
      "  - Minimum Qual Requirements\n",
      "  - Preferred Skills\n",
      "  - Additional Information\n",
      "  - To Apply\n",
      "  - Hours/Shift\n",
      "  - Work Location 1\n",
      "  - Recruitment Contact\n",
      "  - Residency Requirement\n",
      "  - Posting Date\n",
      "  - Post Until\n",
      "  - Posting Updated\n",
      "  - Process Date\n"
     ]
    }
   ],
   "source": [
    "# Column type analysis\n",
    "\n",
    "\n",
    "numerical_cols = [field.name for field in df.schema.fields \n",
    "                 if field.dataType.typeName() in ['integer', 'double', 'float', 'long']]\n",
    "string_cols = [field.name for field in df.schema.fields \n",
    "              if field.dataType.typeName() == 'string']\n",
    "\n",
    "print(f\"\\nNumerical columns:\")\n",
    "for col_name in numerical_cols:\n",
    "    print(f\"  - {col_name}\")\n",
    "\n",
    "print(f\"\\nString columns:\")\n",
    "for col_name in string_cols:\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicate rows: 31\n"
     ]
    }
   ],
   "source": [
    "#Duplicate check\n",
    "exact_dup_count = df.count() - df.dropDuplicates().count()\n",
    "print(f\"Exact duplicate rows: {exact_dup_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Column  Null_Count\n",
      "          Recruitment Contact        1763\n",
      "                   Post Until        1499\n",
      "              Work Location 1        1138\n",
      "                  Hours/Shift        1062\n",
      "        Residency Requirement         678\n",
      "       Additional Information         563\n",
      "                 Posting Date         517\n",
      "              Posting Updated         508\n",
      "                 Process Date         425\n",
      "             Preferred Skills         259\n",
      "Full-Time/Part-Time indicator         195\n",
      "                     To Apply         180\n",
      "    Minimum Qual Requirements          18\n",
      "                 Job Category           2\n"
     ]
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "\n",
    "missing_data = []\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    total_count = df.count()\n",
    "    null_pct = (null_count / total_count) * 100\n",
    "    if null_count > 0:\n",
    "        missing_data.append({\n",
    "            'Column': column,\n",
    "            'Null_Count': null_count\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data).sort_values('Null_Count', ascending=False)\n",
    "#print(missing_df.sort_index().to_string())\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEfore removing duplicates: 2946\n",
      "After removing duplicate: 2915\n",
      "Salary columns cleaned and Average_Salary created\n",
      " Text columns cleaned\n",
      "+--------------------+-----------------+---------------+--------------+\n",
      "|      Business Title|Salary Range From|Salary Range To|Average_Salary|\n",
      "+--------------------+-----------------+---------------+--------------+\n",
      "|  Software Architect|          95000.0|       110000.0|      102500.0|\n",
      "|Quality Analyst L...|          85000.0|        98000.0|       91500.0|\n",
      "|Manager, IT Inven...|          79471.0|       100000.0|       89735.5|\n",
      "|Project Manager -...|          85000.0|        98000.0|       91500.0|\n",
      "| Resiliency Engineer|          85000.0|       105000.0|       95000.0|\n",
      "+--------------------+-----------------+---------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Cleaning - Remove duplicates and clean salary columns - remove $ and commas, convert to numeric\n",
    "\n",
    "print(f\"BEfore removing duplicates: {df.count()}\")\n",
    "df = df.dropDuplicates()\n",
    "print(f\"After removing duplicate: {df.count()}\")\n",
    "\n",
    "salary_cols = ['Salary Range From', 'Salary Range To']\n",
    "\n",
    "for col_name in salary_cols:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            regexp_replace(col(col_name), \"[$,]\", \"\").cast(DoubleType())\n",
    "        )\n",
    "\n",
    "# Create average salary column\n",
    "df = df.withColumn(\n",
    "    'Average_Salary',\n",
    "    (col('Salary Range From') + col('Salary Range To')) / 2\n",
    ")\n",
    "\n",
    "print(\"Salary columns cleaned and Average_Salary created\")\n",
    "\n",
    "# Clean text columns - trim whitespace\n",
    "text_columns = ['Business Title', 'Job Category', 'Agency', \n",
    "               'Civil Service Title', 'Title Classification']\n",
    "\n",
    "for col_name in text_columns:\n",
    "    if col_name in df.columns:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "print(\" Text columns cleaned\")\n",
    "\n",
    "# Show cleaned data sample\n",
    "df.select('Business Title', 'Salary Range From', 'Salary Range To', 'Average_Salary').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|        Job Category|Job_Count|\n",
      "+--------------------+---------+\n",
      "|Engineering, Arch...|      497|\n",
      "|Technology, Data ...|      312|\n",
      "|       Legal Affairs|      224|\n",
      "|Public Safety, In...|      179|\n",
      "|Building Operatio...|      177|\n",
      "|Finance, Accounti...|      168|\n",
      "|Administration & ...|      131|\n",
      "|Constituent Servi...|      129|\n",
      "|              Health|      125|\n",
      "|Policy, Research ...|      124|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Whats the number of jobs posting per category (Top 10)\n",
    "jobs = df.groupBy('Job Category') \\\n",
    "    .agg(count('*').alias('Job_Count')) \\\n",
    "    .orderBy(desc('Job_Count')) \\\n",
    "    .limit(10)\n",
    "\n",
    "jobs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+---------+\n",
      "|        Job Category|Min_Salary|Max_Salary|Job_Count|\n",
      "+--------------------+----------+----------+---------+\n",
      "|Health Policy, Re...|   94889.0|  162500.0|        4|\n",
      "|Administration & ...|   69040.5|   69040.5|        2|\n",
      "|Engineering, Arch...|    539.12|  103050.0|        8|\n",
      "|Administration & ...|   53075.5|   53075.5|        2|\n",
      "|Constituent Servi...|      17.7|  135000.0|      129|\n",
      "|Building Operatio...|      17.7|  169011.0|      177|\n",
      "|Constituent Servi...|   56463.5|   67029.5|        8|\n",
      "|       Legal Affairs|     21.41|  191913.0|      224|\n",
      "|Administration & ...|     30.02|   53094.0|        6|\n",
      "|Maintenance & Ope...|      35.0|   65458.8|        8|\n",
      "|Public Safety, In...|   49399.5|   67021.0|        9|\n",
      "|Administration & ...|  218587.0|  218587.0|        2|\n",
      "|Administration & ...|   70661.0|   70661.0|        2|\n",
      "|Communications & ...|   56867.5|   56867.5|        2|\n",
      "|              Health|     16.28| 170133.84|      125|\n",
      "+--------------------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Whats the salary distribution per job category?\n",
    "sal_grp = df.filter(col('Average_Salary').isNotNull()) \\\n",
    "    .groupBy('Job Category') \\\n",
    "    .agg(\n",
    "        round(min('Average_Salary'),2).alias('Min_Salary'),\n",
    "        round(max('Average_Salary'),2).alias('Max_Salary'),\n",
    "        count('*').alias('Job_Count')\n",
    "    ) \\\n",
    "    .limit(15)\n",
    "\n",
    "sal_grp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+\n",
      "|    Education_Level|        Avg_Salary|Job_Count|\n",
      "+-------------------+------------------+---------+\n",
      "|           Bachelor| 90758.95603053436|      131|\n",
      "|             Master| 84403.69259159022|      654|\n",
      "|Other/Not Specified| 72093.75999386892|      946|\n",
      "|        High School|64665.978645618845|      913|\n",
      "|          Associate| 60497.77874889298|      271|\n",
      "+-------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between the higher degree and the salary\n",
    "df_edu = df.withColumn(\n",
    "    'Education_Level',\n",
    "    when(upper(col('Minimum Qual Requirements')).contains('BACHELOR'), 'Bachelor')\n",
    "    .when(upper(col('Minimum Qual Requirements')).contains('MASTER'), 'Master')\n",
    "    .when(upper(col('Minimum Qual Requirements')).contains('DOCTORAL'), 'Doctoral') \n",
    "    .when(upper(col('Minimum Qual Requirements')).contains('PHD'), 'Doctoral')\n",
    "    .when(upper(col('Minimum Qual Requirements')).contains('ASSOCIATE'), 'Associate')\n",
    "    .when(upper(col('Minimum Qual Requirements')).contains('HIGH SCHOOL'), 'High School')\n",
    "    .otherwise('Other/Not Specified')\n",
    ")\n",
    "\n",
    "correlation = df_edu.filter(col('Average_Salary').isNotNull()) \\\n",
    "    .groupBy('Education_Level') \\\n",
    "    .agg(\n",
    "        avg('Average_Salary').alias('Avg_Salary'),\n",
    "        count('*').alias('Job_Count')\n",
    "    ) \\\n",
    "    .orderBy(desc('Avg_Salary'))\n",
    "\n",
    "correlation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------+--------------------+\n",
      "|Job ID|              Agency|      Business Title|Average_Salary|        Job Category|\n",
      "+------+--------------------+--------------------+--------------+--------------------+\n",
      "|396521|DEPT OF ENVIRONME...|Deputy Commission...|      218587.0|Administration & ...|\n",
      "|415583|   POLICE DEPARTMENT|Deputy Commission...|      217201.0|Constituent Servi...|\n",
      "|425494|DISTRICT ATTORNEY...|Co-Chief Informat...|      191913.0|       Legal Affairs|\n",
      "|391188|NYC HOUSING AUTHO...|executive Vice Pr...|      169011.0|Building Operatio...|\n",
      "|404375|OFFICE OF THE COM...|Head of Short Ter...|      167500.0|Finance, Accounti...|\n",
      "|425828|      LAW DEPARTMENT|Deputy Borough Ch...|      153666.0|       Legal Affairs|\n",
      "|416442|DEPT OF DESIGN & ...|Associate Commiss...|      151795.0|Engineering, Arch...|\n",
      "|415326|DEPARTMENT FOR TH...|Executive Agency ...|      150371.0|       Legal Affairs|\n",
      "|423210|HOUSING PRESERVAT...|Assistant Commiss...|      135000.0|Constituent Servi...|\n",
      "|412080|CONFLICTS OF INTE...|DIRECTOR OF ENFOR...|      135000.0|       Legal Affairs|\n",
      "|422733|FINANCIAL INFO SV...|SENIOR PEOPLESOFT...|      130000.0|Technology, Data ...|\n",
      "|425833|DEPT OF PARKS & R...|Deputy Borough Ch...|      128000.0|Building Operatio...|\n",
      "|396334|DEPARTMENT OF PRO...| IT Security Officer|      127500.0|Technology, Data ...|\n",
      "|399565|DEPT OF INFO TECH...|Senior Director, ...|      127500.0|Technology, Data ...|\n",
      "|420216|     FIRE DEPARTMENT|Senior Enterprise...|      120474.5|Technology, Data ...|\n",
      "+------+--------------------+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Whats the job posting having the highest salary per agency?\n",
    "window_spec = Window.partitionBy('Agency').orderBy(desc('Average_Salary'))\n",
    "\n",
    "job_pos = df.filter(col('Average_Salary').isNotNull()) \\\n",
    "    .withColumn('rank', row_number().over(window_spec)) \\\n",
    "    .filter(col('rank') == 1) \\\n",
    "    .select('Job ID','Agency', 'Business Title', 'Average_Salary', 'Job Category') \\\n",
    "    .orderBy(desc('Average_Salary')) \\\n",
    "    .limit(15)\n",
    "\n",
    "job_pos.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whats the job positings average salary per agency for the last 2 years?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    Preferred Skills|       Work Location|\n",
      "+--------------------+--------------------+\n",
      "| all candidates m...|335 Adams Street,...|\n",
      "|               \"\"2\"\"|59-17 Junction Bl...|\n",
      "|1.\\tMinimum 15 ye...|96-05 Horace Hard...|\n",
      "| to be eligible f...|28-11 Queens Plaz...|\n",
      "| candidates must ...|120 Broadway, New...|\n",
      "| \"\"2\"\" or \"\"3\"\" a...|335 Adams Street,...|\n",
      "|Ability to commun...|   55 Water St Ny Ny|\n",
      "|PREFERRED SKILLS ...|421 East 26th Str...|\n",
      "| individuals must...|421 East 26th Str...|\n",
      "| all candidates m...|600 W 168Th St., ...|\n",
      "|A valid or abilit...|10 Walker Rd, Val...|\n",
      "|QUALIFICATIONS:  ...|255 Greenwich Street|\n",
      "| all candidates m...|100 Church St., N.Y.|\n",
      "| \"\"b\"\" and \"\"c\"\" ...|Office for Exec P...|\n",
      "|Proven working ex...|100 Church St., N.Y.|\n",
      "| including the 18...|96-05 Horace Hard...|\n",
      "| including one ye...|        2 Metro Tech|\n",
      "| all candidates m...|1 Police Plaza, N.Y.|\n",
      "|-\\tGood writing a...|   42-09 28th Street|\n",
      "|We are looking fo...|33 Beaver St, New...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# What are the highest paid skills in the US market?\n",
    "\n",
    "df_skl = df.select(\"Preferred Skills\",\"Work Location\").distinct()\n",
    "df_skl.show(truncate=True)\n",
    "#very noisy data need more time to work on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary_Bucket created\n",
      "Employment_Type standardized\n",
      "+-------------+---------------+--------------------+------------+\n",
      "|Salary_Bucket|Employment_Type|        Posting Date|Posted_since|\n",
      "+-------------+---------------+--------------------+------------+\n",
      "|    Mid Level|      Full-Time|2018-03-27T00:00:...|        2875|\n",
      "|    Mid Level|      Full-Time|2018-04-09T00:00:...|        2862|\n",
      "|  Entry Level|      Full-Time|2016-03-14T00:00:...|        3618|\n",
      "| Senior Level|      Full-Time|2016-12-19T00:00:...|        3338|\n",
      "|    Mid Level|      Full-Time|2017-08-30T00:00:...|        3084|\n",
      "|    Mid Level|      Full-Time|2018-05-02T00:00:...|        2839|\n",
      "|  Entry Level|      Full-Time|2018-06-25T00:00:...|        2785|\n",
      "|  Entry Level|  Not Specified|2015-07-14T00:00:...|        3862|\n",
      "| Senior Level|      Full-Time|2017-08-15T00:00:...|        3099|\n",
      "|  Entry Level|      Full-Time|2017-11-01T00:00:...|        3021|\n",
      "+-------------+---------------+--------------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Created a salary bucket for simplification\n",
    "df = df.withColumn(\n",
    "    'Salary_Bucket',\n",
    "    when(col('Average_Salary') < 50000, 'Entry Level')\n",
    "    .when((col('Average_Salary') >= 50000) & (col('Average_Salary') < 80000), 'Mid Level')\n",
    "    .when((col('Average_Salary') >= 80000) & (col('Average_Salary') < 120000), 'Senior Level')\n",
    "    .when(col('Average_Salary') >= 120000, 'Executive Level')\n",
    "    .otherwise('Not Specified')\n",
    ")\n",
    "print(\"Salary_Bucket created\")\n",
    "\n",
    "\n",
    "# Employment Type Standardization\n",
    "df = df.withColumn(\n",
    "    'Employment_Type',\n",
    "    when(upper(col('Full-Time/Part-Time indicator')).contains('F'), 'Full-Time')\n",
    "    .when(upper(col('Full-Time/Part-Time indicator')).contains('P'), 'Part-Time')\n",
    "    .otherwise('Not Specified')\n",
    ")\n",
    "print(\"Employment_Type standardized\")\n",
    "\n",
    "# Linkedin like Posted Since \n",
    "df = df.filter(\n",
    "    trim(col('Posting Date')).rlike('^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}$')\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    'format_date', \n",
    "    to_timestamp(trim(col('Posting Date')), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    ").withColumn(\n",
    "    'Posted_since',\n",
    "    datediff(current_date(), col('format_date'))\n",
    ")\n",
    "\n",
    "\n",
    "# Show sample with new features\n",
    "df.select(\n",
    "    'Salary_Bucket', \n",
    "    'Employment_Type',\n",
    "    'Posting Date',\n",
    "    'Posted_since'\n",
    ").show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3468.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/workspace/processed_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save as CSV with single partition\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed data saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal records saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\APPLICATIONS\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:2146\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   2129\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2130\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2144\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   2145\u001b[0m )\n\u001b[1;32m-> 2146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\APPLICATIONS\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\APPLICATIONS\\Anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\APPLICATIONS\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3468.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t\t... 1 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/opt/workspace/processed_data\"\n",
    "\n",
    "# Save as CSV with single partition\n",
    "df.coalesce(1).write.mode('overwrite').option('header', 'true').csv(output_path)\n",
    "\n",
    "print(f\"Processed data saved to: {output_path}\")\n",
    "print(f\"Total records saved: {df.count()}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "\n",
    "#Getting error while writing \n",
    "#Py4JJavaError: An error occurred while calling o3468.csv.\n",
    "#: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
